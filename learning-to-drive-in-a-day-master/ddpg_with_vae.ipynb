{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2018 Roma Sokolkov\n",
    "# MIT License\n",
    "\n",
    "\"\"\"\n",
    "DDPGWithVAE inherits DDPG from stable-baselines\n",
    "and reimplements learning method.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.ddpg.ddpg import DDPG\n",
    "\n",
    "\n",
    "class DDPGWithVAE(DDPG):\n",
    "    \"\"\"\n",
    "    Modified learn method from stable-baselines\n",
    "\n",
    "    - Stop rollout on episode done.\n",
    "    - More verbosity.\n",
    "    - Add VAE optimization step.\n",
    "    \"\"\"\n",
    "    def learn(self, total_timesteps, callback=None, vae=None, skip_episodes=5):\n",
    "        rank = MPI.COMM_WORLD.Get_rank()\n",
    "        # we assume symmetric actions.\n",
    "        assert np.all(np.abs(self.env.action_space.low) == self.env.action_space.high)\n",
    "\n",
    "        self.episode_reward = np.zeros((1,))\n",
    "        with self.sess.as_default(), self.graph.as_default():\n",
    "            # Prepare everything.\n",
    "            self._reset()\n",
    "            episode_reward = 0.\n",
    "            episode_step = 0\n",
    "            episodes = 0\n",
    "            step = 0\n",
    "            total_steps = 0\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            actor_losses = []\n",
    "            critic_losses = []\n",
    "\n",
    "            while True:\n",
    "                obs = self.env.reset()\n",
    "                # Rollout one episode.\n",
    "                while True:\n",
    "                    if total_steps >= total_timesteps:\n",
    "                        return self\n",
    "\n",
    "                    # Predict next action.\n",
    "                    action, q_value = self._policy(obs, apply_noise=True, compute_q=True)\n",
    "                    print(action)\n",
    "                    assert action.shape == self.env.action_space.shape\n",
    "\n",
    "                    # Execute next action.\n",
    "                    if rank == 0 and self.render:\n",
    "                        self.env.render()\n",
    "                    new_obs, reward, done, _ = self.env.step(action * np.abs(self.action_space.low))\n",
    "\n",
    "                    step += 1\n",
    "                    total_steps += 1\n",
    "                    if rank == 0 and self.render:\n",
    "                        self.env.render()\n",
    "                    episode_reward += reward\n",
    "                    episode_step += 1\n",
    "\n",
    "                    # Book-keeping.\n",
    "                    # Do not record observations, while we skip DDPG training.\n",
    "                    if (episodes + 1) > skip_episodes:\n",
    "                        self._store_transition(obs, action, reward, new_obs, done)\n",
    "                    obs = new_obs\n",
    "                    if callback is not None:\n",
    "                        callback(locals(), globals())\n",
    "\n",
    "                    if done:\n",
    "                        print(\"episode finished. Reward: \", episode_reward)\n",
    "                        # Episode done.\n",
    "                        episode_reward = 0.\n",
    "                        episode_step = 0\n",
    "                        episodes += 1\n",
    "\n",
    "                        self._reset()\n",
    "                        obs = self.env.reset()\n",
    "                        # Finish rollout on episode finish.\n",
    "                        break\n",
    "\n",
    "                print(\"rollout finished\")\n",
    "\n",
    "                # Train VAE.\n",
    "                train_start = time.time()\n",
    "                vae.optimize()\n",
    "                print(\"VAE training duration:\", time.time() - train_start)\n",
    "\n",
    "                # Train DDPG.\n",
    "                actor_losses = []\n",
    "                critic_losses = []\n",
    "                train_start = time.time()\n",
    "                if episodes > skip_episodes:\n",
    "                    for t_train in range(self.nb_train_steps):\n",
    "                        critic_loss, actor_loss = self._train_step(0, None, log=t_train == 0)\n",
    "                        critic_losses.append(critic_loss)\n",
    "                        actor_losses.append(actor_loss)\n",
    "                        self._update_target_net()\n",
    "                    print(\"DDPG training duration:\", time.time() - train_start)\n",
    "\n",
    "                    mpi_size = MPI.COMM_WORLD.Get_size()\n",
    "                    # Log stats.\n",
    "                    # XXX shouldn't call np.mean on variable length lists\n",
    "                    duration = time.time() - start_time\n",
    "                    stats = self._get_stats()\n",
    "                    combined_stats = stats.copy()\n",
    "                    combined_stats['train/loss_actor'] = np.mean(actor_losses)\n",
    "                    combined_stats['train/loss_critic'] = np.mean(critic_losses)\n",
    "                    combined_stats['total/duration'] = duration\n",
    "                    combined_stats['total/steps_per_second'] = float(step) / float(duration)\n",
    "                    combined_stats['total/episodes'] = episodes\n",
    "\n",
    "                    def as_scalar(scalar):\n",
    "                        \"\"\"\n",
    "                        check and return the input if it is a scalar, otherwise raise ValueError\n",
    "\n",
    "                        :param scalar: (Any) the object to check\n",
    "                        :return: (Number) the scalar if x is a scalar\n",
    "                        \"\"\"\n",
    "                        if isinstance(scalar, np.ndarray):\n",
    "                            assert scalar.size == 1\n",
    "                            return scalar[0]\n",
    "                        elif np.isscalar(scalar):\n",
    "                            return scalar\n",
    "                        else:\n",
    "                            raise ValueError('expected scalar, got %s' % scalar)\n",
    "\n",
    "                    combined_stats_sums = MPI.COMM_WORLD.allreduce(\n",
    "                        np.array([as_scalar(x) for x in combined_stats.values()]))\n",
    "                    combined_stats = {k: v / mpi_size for (k, v) in zip(combined_stats.keys(), combined_stats_sums)}\n",
    "\n",
    "                    # Total statistics.\n",
    "                    combined_stats['total/steps'] = step\n",
    "\n",
    "                    for key in sorted(combined_stats.keys()):\n",
    "                        logger.record_tabular(key, combined_stats[key])\n",
    "                    logger.dump_tabular()\n",
    "                    logger.info('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
